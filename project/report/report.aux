\relax 
\citation{nature_dqn}
\citation{1509.06461}
\citation{1511.06581}
\citation{deepmind}
\citation{open.ai}
\citation{nature_dqn}
\citation{nature_dqn}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduzione}{1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Double Dueling Deep Q-learning}{1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Deep Q-Learning}{1}\protected@file@percent }
\citation{1511.06581}
\citation{1511.06581}
\citation{1511.06581}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces DQN algortihm presentato nell'articolo \cite  {nature_dqn}}}{2}\protected@file@percent }
\newlabel{fig:dqn_algo}{{1}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Double Q-learning}{2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Dueling DDQN}{2}\protected@file@percent }
\citation{open.ai}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Immagine presa dall'articolo \cite  {1511.06581}. Viene mostrata la differenza tra l'archittettura classica (in alto) con diversi strati convoluzionali e uno strato fully connected e l'archittetura proposta (in basso) che sdoppia l'ultimo strato in due flussi: il primo risulta in uno scalare e rappresenta il valore (\textit  {value}) dello stato e il secondo risulta nei vantaggi (advantage) per ogni azione}}{3}\protected@file@percent }
\newlabel{fig:dueling_arch}{{2}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Ambiente OpenAI Gym}{3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Fermo immagine di una sessione per il task CartPole-v1 di OpenAI Gym}}{3}\protected@file@percent }
\newlabel{fig:cartpole}{{3}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Implementazione}{4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Architettura del modello}{4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Algoritmo}{4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Tecnologie utilizzate e dettagli implementativi}{5}\protected@file@percent }
\citation{git}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Decadimento esponenziale di $\epsilon $ con reset del ciclo}}{6}\protected@file@percent }
\newlabel{fig:epsilon}{{4}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Risultati}{6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Parametri utilizzati}{6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Training e reward totali}{6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Andamento del reward totale guadagnato dall'agente in fase di training}}{7}\protected@file@percent }
\newlabel{fig:rewards}{{5}{7}}
\bibdata{mybib}
\bibcite{deepmind}{1}
\bibcite{open.ai}{2}
\bibcite{git}{3}
\bibcite{nature_dqn}{4}
\bibcite{1509.06461}{5}
\bibcite{1511.06581}{6}
\bibstyle{plain}
